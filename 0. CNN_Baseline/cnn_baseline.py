# -*- coding: utf-8 -*-
"""CNN_Baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hVVJJL6xBatlW9lpV7PbX6cMBjE6uh3K
"""

# --- Step 0: environment and folders ------------------------------------
!pip install --quiet torch torchvision timm psutil

from pathlib import Path
import os, json, psutil, time
import torch, torch.nn as nn, torch.optim as optim
import torchvision.transforms as T
import torchvision.datasets as datasets
import torchvision.models as models

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Base project directory inside Drive
BASE_DIR = Path('/content/drive/MyDrive/ANN_Project')
CNN_DIR  = BASE_DIR / 'CNN_Baseline'
MODEL_DIR   = CNN_DIR / 'model'
METRIC_DIR  = CNN_DIR / 'metrics'
for d in [MODEL_DIR, METRIC_DIR]:
    d.mkdir(parents=True, exist_ok=True)

print(f"Folders ready:\n  {MODEL_DIR}\n  {METRIC_DIR}")

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Running on:', device)

# --- Step 1: dataset + dataloaders --------------------------------------
from torch.utils.data import DataLoader
import torchvision.transforms as T
import torchvision.datasets as datasets

# Normalisation statistics for CIFAR‑100 (pre‑computed)
mean = (0.5071, 0.4867, 0.4408)
std  = (0.2675, 0.2565, 0.2761)

# Data augmentation / preprocessing
train_tfms = T.Compose([
    T.RandomCrop(32, padding=4),
    T.RandomHorizontalFlip(),
    T.ToTensor(),
    T.Normalize(mean, std),
])

test_tfms = T.Compose([
    T.ToTensor(),
    T.Normalize(mean, std),
])

# Download (first time) or load from cache
data_root = '/content/data'          # Colab VM path (small; OK for CIFAR)
train_ds  = datasets.CIFAR100(root=data_root, train=True,  download=True, transform=train_tfms)
test_ds   = datasets.CIFAR100(root=data_root, train=False, download=True, transform=test_tfms)

# DataLoaders
train_dl = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)
test_dl  = DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=2, pin_memory=True)

print(f"Train set: {len(train_ds)} images\nTest  set: {len(test_ds)} images")

# --- Step 2: model + optimiser + scheduler ------------------------------
import torch.nn as nn
from torch.optim.lr_scheduler import CosineAnnealingLR

# 1. Build ResNet‑34 with ImageNet weights and replace the final FC
model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)
model.fc = nn.Linear(model.fc.in_features, 100)   # CIFAR‑100 has 100 classes
model = model.to(device)

# enable CuDNN autotuner for a speed boost on fixed-size inputs
torch.backends.cudnn.benchmark = True

# 2. Loss function
criterion = nn.CrossEntropyLoss()

# 3. Optimiser (SGD + momentum)
optimizer = optim.SGD(model.parameters(), lr=0.1,
                      momentum=0.9, weight_decay=5e-4)

# 4. Cosine LR scheduler over 50 epochs
scheduler = CosineAnnealingLR(optimizer, T_max=50)

# 5. Convenience: parameter count
param_cnt = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Model ready: ResNet‑34  |  Trainable params: {param_cnt/1e6:.2f} M")

# --- Step 3: training loop ------------------------------------------------
from collections import defaultdict
import numpy as np

NUM_EPOCHS = 50
history = defaultdict(list)         # store metrics for later JSON dump

def accuracy(model, loader):
    model.eval()
    correct = 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            correct += (model(x).argmax(1) == y).sum().item()
    return correct / len(loader.dataset)

best_acc = 0.0
start_total = time.time()

for epoch in range(1, NUM_EPOCHS + 1):
    epoch_start = time.time()
    model.train()

    # Reset CUDA peak‑memory counter
    if device == 'cuda':
        torch.cuda.reset_peak_memory_stats(device)

    running_loss = 0.0
    for x, y in train_dl:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * x.size(0)

    scheduler.step()

    train_loss = running_loss / len(train_ds)
    val_acc    = accuracy(model, test_dl)
    gpu_peak   = torch.cuda.max_memory_allocated(device) / 1024**2 if device == 'cuda' else 0
    epoch_time = time.time() - epoch_start

    # Record metrics
    history['epoch'].append(epoch)
    history['train_loss'].append(round(train_loss, 4))
    history['val_acc'].append(round(val_acc, 4))
    history['epoch_sec'].append(round(epoch_time, 1))
    history['gpu_mb'].append(int(gpu_peak))

    if val_acc > best_acc:
        best_acc = val_acc
        # Keep a copy of best model weights in memory; we’ll save to Drive later
        best_state_dict = model.state_dict()

    print(f"E{epoch:02d}: loss={train_loss:.3f}  acc={val_acc:.3%}  "
          f"time={epoch_time:.1f}s  peakVRAM={gpu_peak:.0f} MB")

total_time = time.time() - start_total
print(f"\nTraining finished in {total_time/60:.1f} min — best val acc: {best_acc:.3%}")

# --- Step 4: persist best model + metrics -------------------------------
import torch
import json
from datetime import datetime

stamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# 1. Save best model weights
model_path = MODEL_DIR / f'resnet34_cifar100_best_{stamp}.pth'
torch.save(best_state_dict, model_path)
print(f"✅ Model saved to: {model_path}")

# 2. Save training history as JSON
metrics_path = METRIC_DIR / f'resnet34_cifar100_metrics_{stamp}.json'
with open(metrics_path, 'w') as f:
    json.dump(history, f, indent=2)
print(f"✅ Metrics saved to: {metrics_path}")

# 3. Quick summary line for your lab notebook
print(f"Baseline ResNet‑34 → best acc = {best_acc:.3%} (epoch {history['val_acc'].index(round(best_acc,4))+1})")

